---
title: "Alina: Bias"
nextjs:
  metadata:
    title: "Alina: Bias"
    description: TBD.
---

{% callout title="Early Draft" %}
***Note***: This page is an early draft of potential case studies. It has not yet been edited or throughly reviewed.
{% /callout %}

## 1. Coded Bias

| Time Stamp | Scene Description                                           | Type of Bias             | Notes                                                                 |
|------------|-------------------------------------------------------------|--------------------------|-----------------------------------------------------------------------|
| 10:35      | Joy Buolamwini discovered facial recognition errors.        | Racial and Gender Bias   | Shows how AI fails to correctly distinguish dark-skinned and feminine faces. |
| 22:11      | Interviews with professionals address algorithmic bias.     | General Algorithm Bias   | Experts examine how biases in AI influence different minority groups. |
| 45:40      | Footage from the UK, where facial recognition was used by police | Racial Bias              | Shows how people of color are disproportionately misidentified by AI. |
| 1:05:09    | Activists and legislators call for regulation of AI         | Institutional Bias       | Demonstrates the need for legal frameworks to prevent bias in AI deployment. |
| 1:23:38    | Examination of AI applications in hiring processes          | Employment Bias          | Discusses how biased algorithms can disadvantage certain demographics in job markets. |
| 1:50:11    | Joy Buolamwini’s presentation to Congress                   | Advocacy against Bias    | Joy argues for regulations that confront and reduce biases in AI.     |


## 2. Facial Recognition Leads to Wrongful Arrest (Detroit)

“Face Recognition Led to Wrongful Arrest” - The New York Times (news)

**Summary / The Facts:** Facial recognition technology used by the Detroit Police Department has led to wrongful arrests, including those of Robert Williams and Porcha Woodruff. As part of a legal settlement, Detroit has adopted new rules for its use to prevent such errors in the future.

**Benefits:** Facial recognition technology can help solve crimes by identifying suspects quickly, especially in serious cases like assault, murder, and home invasions.

**Harms:** The technology has led to wrongful arrests, primarily affecting people of color, and has raised concerns about privacy, racial bias, and the reliability of facial recognition matches.

**Who Benefits?** Law enforcement agencies benefit from the technology's potential to solve crimes quickly. Companies providing facial recognition systems also benefit from their sales and contracts with police departments.

**Who is Harmed?** Individuals misidentified by the technology, particularly people of color, are harmed. This includes wrongful arrests, emotional distress, and potential long-term impacts on their records and reputations.

**Stakeholders — Direct and Indirect:**

- Direct: Wrongfully accused individuals, police departments, technology providers.
- Indirect: Communities impacted by policing practices, legal and civil rights organizations, policymakers, and the general public.

**Connections to Artificial Cerebellar Circuit (ACCs):** Facial recognition technology and ACCs both involve advanced algorithms and neural network designs. ACCs, being part of machine learning and AI, relate to how facial recognition software processes and matches images.

**Similar Cases:** Similar wrongful arrest cases have occurred in Louisiana, New Jersey, Maryland, and Texas. Concerns about facial recognition are also prevalent in San Francisco, Austin, and Portland, where bans or limitations have been implemented.

**Themes:**

- Racial bias in technology
- Privacy and surveillance
- Accountability in law enforcement
- Ethical use of AI

**Precedence:** The cases in Detroit and elsewhere set a precedent for legal challenges against the use of facial recognition technology by law enforcement and push for stricter regulations or bans to protect civil rights.

**References**

* Hill, Kashmir. “Facial Recognition Led to Wrongful Arrests. So Detroit Is Making Changes.” The New York Times, 29 June 2024, [https://www.nytimes.com/2024/06/29/technology/detroit-facial-recognition-false-arrests.html](https://www.nytimes.com/2024/06/29/technology/detroit-facial-recognition-false-arrests.html)
* O'BRIEN, MATT. “Detroit police challenged over face recognition flaws, bias.” *AP News*, 24 June 2020, [https://apnews.com/article/us-news-ap-top-news-theft-arrests-mi-state-wire-9406d44edad083ee04e28646ead58ec7](https://apnews.com/article/us-news-ap-top-news-theft-arrests-mi-state-wire-9406d44edad083ee04e28646ead58ec7)


## 3. Google Translate Gender Bias Case

**Summary / The Facts:** Google Translate has shown gender prejudice, particularly in translations from languages that employ gender-neutral pronouns, such as Turkish. For example, "o bir doktor" ("he/she is a doctor") was commonly translated as "he is a doctor," and "o bir hemşire" ("he/she is a nurse") was translated as "she is a nurse." This bias results from societal prejudices found in the training data. To solve this, Google has incorporated gender-specific translations, which include both male and feminine variants where appropriate.

**Benefits:** This initiative enhances translation accuracy and ensures that translations are contextually appropriate, reducing the reinforcement of gender stereotypes. By addressing such biases, Google improves user trust and demonstrates a commitment to fairness and inclusivity in its AI products.

**Harms:** The process of adding gender-specific translations introduces complexity and may lead to potential delays in translation services due to additional processing steps. Despite these efforts, the solution might not fully eliminate all forms of bias present in translations, indicating that more work is needed to achieve complete fairness.

**Who Benefits?** Users benefit from more accurate and fair translations. Marginalized groups, often misrepresented in AI translations, gain better representation. Developers and researchers gain valuable insights into mitigating bias in AI systems.

**Who is Harmed?** Users might experience slight delays in receiving translations. Developers face increased complexity in maintaining and updating translation models to accommodate these changes.

**Stakeholders — Direct and Indirect:** Direct stakeholders include Google Translate users, developers, and AI researchers. Indirect stakeholders surround the broader society, especially communities concerned with gender representation and equality.

**Connections to Artificial Cerebellar Circuit (ACCs):** While not directly connected, this case relates to broader issues of bias and fairness in AI, similar to those addressed by Artificial Cerebellar Circuits (ACCs).

**Similar Cases:** Other examples of AI bias include Amazon's AI recruiting tool, which exhibited gender bias, and word embeddings in various NLP applications, which encoded gender stereotypes.

**Themes:** The primary themes include addressing bias and fairness in AI systems, ensuring diverse and fair representation in technology, and the need for transparency in AI processes.

**Precedence:** Google’s public acknowledgment and efforts to address AI biases set a precedent for transparency. Implementing gender-specific translations showcases an innovative approach to bias mitigation and sets a standard for other AI products.

**References**
* Johnson, Melvin. “A Scalable Approach to Reducing Gender Bias in Google Translate.” Google Research, 22 April 2020, [https://research.google/blog/a-scalable-approach-to-reducing-gender-bias-in-google-translate/]()
* Marking, Marion. “Google Translate Still Shows Gender Bias. Here's What Google Is Doing About It.” *Slator*, 5 July 2021, [https://slator.com/google-translate-still-shows-gender-bias-heres-what-google-is-doing-about-it/]()


## 4. Wikipedia's ORES Bias Case

**Summary / The Facts:** Wikipedia's Objective Revision Evaluation Service (ORES) is an AI tool used to classify the quality of Wikipedia edits. Studies have shown that ORES disproportionately flags contributions from new editors and anonymous users as potentially damaging, which particularly affects edits from certain geographic regions and non-Western languages. These biases can discourage participation from underrepresented communities and perpetuate existing disparities in content representation.

**Benefits:** ORES improves the efficiency and scalability of detecting potential vandalism, assisting human editors in maintaining the quality and integrity of Wikipedia's content. This automation helps manage the vast volume of edits on the platform, ensuring quicker responses to harmful edits.

**Harms:** The system's bias against certain regions and new editors leads to higher rates of false-positive flags, causing wrongful accusations and discouraging participation from these groups. This can result in emotional distress and hinder the growth of a diverse and inclusive editing community on Wikipedia.

**Who Benefits?** Wikipedia editors benefit from automated assistance in identifying and managing vandalism, while the Wikipedia community overall maintains higher content quality. AI researchers also gain insights into the impacts of bias in automated systems.

- TODO: why are bots useful? What are some “famous” examples of sabotage or examples of other things that happen when quality control bots.
    - [19 Chatbot Examples to Know | Built In](https://builtin.com/artificial-intelligence/chatbot-examples)
    - Bots are useful in ways that incorporate tasks that are repetitive, helping save time for customers and clients, as well as reducing labor costs

**Who is Harmed?** New and anonymous editors face higher rates of false-positive flags, and underrepresented communities experience bias against their contributions. Wikipedia readers might miss out on diverse and accurate content due to these biases.

- Having some stories of what happens when whole countries / perspectives / genders / etc. get banned?
- Some articles that might be helpful: [https://stuartgeiger.com/articles/](https://stuartgeiger.com/articles/)
- [https://memeburn.com/2018/06/wikipedia-wikimania-africa-numbers/](https://memeburn.com/2018/06/wikipedia-wikimania-africa-numbers/)
- [Wikimedia Report (July, 2020)](https://upload.wikimedia.org/wikipedia/commons/6/66/Article_One_Wikimedia_Foundation_July_2020_HRIA_%28English%29.pdf)

**Stakeholders — Direct and Indirect:** Direct stakeholders include Wikipedia editors, new and anonymous contributors, and ORES developers. Indirect stakeholders encompass Wikipedia readers, AI ethics researchers, and global editing communities.

**Connections to Artificial Cerebellar Circuit (ACCs):** While not directly connected, this case is related to broader issues of bias and fairness in AI, similar to those addressed by Artificial Cerebellar Circuits (ACCs).

**Similar Cases:** Similar wrongful arrest cases involving AI bias include the COMPAS algorithm in the U.S. judicial system, which has been shown to be biased against Black defendants. Content moderation tools on social media platforms also exhibit biases against certain content or user groups.

**Themes:** Key themes include addressing bias and fairness in AI moderation tools, encouraging diverse participation in online communities, and the need for transparency in AI processes.

**Precedence:** Highlighting biases in ORES and working towards solutions sets a precedent for transparency. Ensuring fair and accurate AI tools for large-scale community platforms like Wikipedia emphasizes the importance of inclusivity and accountability in AI applications.

**References** 
* Reynolds, Matt. “Wikipedia 'facts' depend on which language you read them in.” New Scientist, 13 December 2016, [https://www.newscientist.com/article/2115986-wikipedia-facts-depend-on-which-language-you-read-them-in/](https://www.newscientist.com/article/2115986-wikipedia-facts-depend-on-which-language-you-read-them-in/)
* [Research:Exploring systematic bias in ORES - Meta [wikimedia.org](https://meta.wikimedia.org/wiki/Research:Exploring_systematic_bias_in_ORES)
